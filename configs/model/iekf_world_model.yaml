# IEKF with LatentWorldModel: probabilistic latent variable world model.
#
# The LatentWorldModel replaces the GRU-based WorldModel with a VAE-style
# encoder that outputs a distribution q(z|x), plus two MLP decoders:
#   - MeasurementDecoder: z → N_n (measurement covariance)
#   - ProcessDecoder:     z → delta_b_omega, delta_b_a, Q_bias_scale
#
# KL regularization term: Loss = t_rel + kl_weight * KL(q(z|x) || N(0,I))
# with linear annealing from 0 to kl_weight over kl_anneal_epochs epochs.

name: "iekf_world_model"

# ------------------------------------------------------------------
# State / physics: identical to iekf_learned_cov
# ------------------------------------------------------------------
state_dim:
  rotation: 9
  velocity: 3
  position: 3
  bias_gyro: 3
  bias_acc: 3
  rotation_c_i: 9
  translation_c_i: 3
  total: 33

P_dim: 21

physics:
  gravity: [0.0, 0.0, -9.80655]

  process_noise:
    cov_omega: 2.0e-4
    cov_acc: 1.0e-3
    cov_b_omega: 1.0e-8
    cov_b_acc: 1.0e-6
    cov_Rot_c_i: 1.0e-8
    cov_t_c_i: 1.0e-8

  initial_covariance:
    cov_Rot0: 1.0e-6
    cov_v0: 1.0e-1
    cov_b_omega0: 1.0e-8
    cov_b_acc0: 1.0e-3
    cov_Rot_c_i0: 1.0e-5
    cov_t_c_i0: 1.0e-2

  measurement_noise:
    cov_lat: 1.0
    cov_up: 10.0

# ------------------------------------------------------------------
# Networks
# ------------------------------------------------------------------
networks:
  # InitProcessCovNet – always on (learns P0 and base Q)
  init_process_cov:
    enabled: true
    type: "InitProcessCovNet"
    architecture:
      output_dim: 6
      initial_beta: 3.0
      weight_scale: 10.0

  # Standalone nets – disabled when world model decoders subsume them
  measurement_cov:
    enabled: false
    type: "MeasurementCovNet"

  bias_correction:
    enabled: false
    type: "LearnedBiasCorrectionNet"

  # Latent variable world model – probabilistic encoder + two MLP decoders
  world_model:
    enabled: true
    type: "LatentWorldModel"

    # KL regularization
    kl_weight: 1.0e-4       # lambda in: Loss = t_rel + lambda * KL(q(z|x)||N(0,I))
    kl_anneal_epochs: 1    # linear anneal from 0 to kl_weight over this many epochs

    architecture:
      # Shared CNN encoder backbone
      input_channels: 6       # [gyro_xyz, acc_xyz]
      cnn_channels: 32
      kernel_size: 5
      cnn_dilation: 3
      cnn_dropout: 0.1 # from 0.5
      latent_dim: 8           # dimension of z; start small to avoid KITTI overfitting

      # MeasurementDecoder: z → N_n (replaces standalone MeasurementCovNet)
      # Output: cov0_measurement * 10^(beta * tanh(raw))
      # When disabled, measurement_cov standalone net is used.
      measurement_decoder:
        enabled: true
        beta: 3.0             # matches AI-IMU beta parameter

      # ProcessDecoder: z → delta_b_omega, delta_b_a, Q_bias_scale
      # Bias corrections bounded by alpha * sigma_{bw,ba} (physics-motivated)
      # Q_bias_scale scales both b_omega and b_acc diagonal elements
      process_decoder:
        enabled: true
        alpha: 3.0            # bound: corrections within ±alpha*sigma_{bw,ba}

      # Near-zero init so decoders start as identity (no correction initially)
      weight_scale: 0.1 # from 0.01
      bias_scale: 0.1 # from 0.01

# ------------------------------------------------------------------
# Optimizer param-groups for the world model
# ------------------------------------------------------------------
optimizer:
  param_groups:
    init_process_cov_net:
      lr: 1.0e-4
    world_model:
      lr: 1.0e-4
      weight_decay: 0.0
