# Default Training Configuration

# Training epochs
epochs: 400
max_loss: 2.0
# Sequence length for training (in samples at 100 Hz)
# 6000 samples = 60 seconds
seq_dim: 6000

# Batch size: number of sequences sampled WITH REPLACEMENT per optimizer step.
# The paper uses 9 (one step per epoch covers ~same compute as legacy 8-sequence loop).
batch_size: 9

# Steps per epoch: how many optimizer steps to take per epoch.
# null = auto: ceil(n_train_sequences / batch_size), matching legacy total-sequence count.
steps_per_epoch: null

# Truncated Backpropagation Through Time (TBPTT)
# When enabled, each sequence is divided into non-overlapping chunks.
# An optimizer step is taken after each chunk; filter state is detached
# at chunk boundaries to limit gradient graph depth.
# Set enabled: false to use the standard (full-sequence) mode.
bptt:
    enabled: true
    chunk_size: 2000 # timesteps per chunk (500 = 5 s at 100 Hz)

# Gradient clipping
gradient_clipping:
    enabled: true
    max_norm: 1.0

save_interval: 10
# Validation
validation:
    enabled: true
    interval: ${training.save_interval} # Validate every N epochs
    save_best: true # Save checkpoint when validation improves
    # Run full filter evaluation on val sequences during training
    test_eval:
        enabled: true
        interval: ${training.save_interval} # Evaluate every N epochs

# Checkpointing
checkpointing:
    enabled: true
    save_interval: ${training.save_interval} # Save checkpoint every N epochs
    keep_last_n: 5 # Keep only last N checkpoints
    save_best: true # Always keep best checkpoint

# Resume from checkpoint
resume:
    enabled: false
    checkpoint_path: null
    reset_optimizer: false # If true, create new optimizer instead of loading

# Debug mode
debug:
    fast_dev_run: false # Run 1 batch per epoch for testing
    overfit_batches: 0 # If > 0, overfit on N batches
    log_frequency: ${training.save_interval} # Log every N batches

# Random seed for reproducibility
seed: 42

# Device
device: "cuda" # "cuda" or "cpu", will auto-fallback to cpu if cuda unavailable

# Mixed precision training
use_amp: false # Automatic Mixed Precision (experimental)
